---
# LLM Inference Server (NVIDIA NIM or vLLM)
apiVersion: v1
kind: Secret
metadata:
  name: llm-secrets
  namespace: voice-workflow
type: Opaque
stringData:
  # NGC API key for NIM
  ngc-api-key: ""
  # HuggingFace token for model download
  hf-token: ""
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-config
  namespace: voice-workflow
data:
  # Model configuration
  MODEL_NAME: "meta/llama-3.1-8b-instruct"
  MAX_MODEL_LEN: "4096"
  GPU_MEMORY_UTILIZATION: "0.9"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nim-llm
  namespace: voice-workflow
  labels:
    app: nim-llm
    app.kubernetes.io/name: nim-llm
    app.kubernetes.io/component: inference
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nim-llm
  template:
    metadata:
      labels:
        app: nim-llm
    spec:
      imagePullSecrets:
        - name: ngc-secret
      containers:
        - name: nim-llm
          # Using NVIDIA NIM container for optimized inference
          image: nvcr.io/nim/meta/llama-3.1-8b-instruct:1.2.2
          ports:
            - containerPort: 8000
              name: http
          env:
            - name: NGC_API_KEY
              valueFrom:
                secretKeyRef:
                  name: llm-secrets
                  key: ngc-api-key
            - name: NIM_MAX_MODEL_LEN
              valueFrom:
                configMapKeyRef:
                  name: llm-config
                  key: MAX_MODEL_LEN
          resources:
            limits:
              nvidia.com/gpu: 1
              memory: "48Gi"
              cpu: "16"
            requests:
              nvidia.com/gpu: 1
              memory: "32Gi"
              cpu: "8"
          volumeMounts:
            - name: nim-cache
              mountPath: /opt/nim/.cache
          livenessProbe:
            httpGet:
              path: /v1/health/live
              port: 8000
            initialDelaySeconds: 600
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /v1/health/ready
              port: 8000
            initialDelaySeconds: 300
            periodSeconds: 10
      volumes:
        - name: nim-cache
          persistentVolumeClaim:
            claimName: nim-cache-pvc
      nodeSelector:
        nvidia.com/gpu.present: "true"
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
---
apiVersion: v1
kind: Service
metadata:
  name: nim-llm
  namespace: voice-workflow
  labels:
    app: nim-llm
spec:
  type: ClusterIP
  ports:
    - port: 8000
      targetPort: 8000
      name: http
  selector:
    app: nim-llm
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nim-cache-pvc
  namespace: voice-workflow
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
  storageClassName: standard

