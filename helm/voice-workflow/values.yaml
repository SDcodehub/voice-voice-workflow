# =============================================================================
# Voice Workflow Helm Chart - Configuration Values
# =============================================================================
#
# This file contains all configurable values for the Voice-to-Voice Workflow.
# Modify these values to customize your deployment without changing templates.
#
# Usage:
#   helm upgrade --install voice-gateway helm/voice-workflow -n voice-workflow
#
# Override specific values:
#   helm upgrade ... --set config.llm.temperature=0.7
#   helm upgrade ... --set gateway.replicas=3
#
# =============================================================================

# -----------------------------------------------------------------------------
# Global Configuration
# -----------------------------------------------------------------------------
global:
  ngcSecret: "ngc-secret"   # Kubernetes secret containing NGC_API_KEY

# =============================================================================
# RUNTIME CONFIGURATION (ConfigMap)
# =============================================================================
# These values are injected as environment variables via ConfigMap.
# Changes here do NOT require rebuilding the container image.
# After changing, redeploy with: ./scripts/deploy_gateway.sh
# -----------------------------------------------------------------------------
config:
  # ---------------------------------------------------------------------------
  # ASR (Automatic Speech Recognition) Configuration
  # ---------------------------------------------------------------------------
  # Controls how the Riva ASR service processes incoming audio
  asr:
    language: "en-US"         # Language code for speech recognition
                              # Supported: en-US, hi-IN (requires Hindi model)
                              # Must match the deployed Riva ASR model
    sampleRate: "16000"       # Audio sample rate in Hz
                              # Standard: 16000 (telephony/voice), 44100 (music)
                              # Must match client recording settings

  # ---------------------------------------------------------------------------
  # LLM (Large Language Model) Configuration
  # ---------------------------------------------------------------------------
  # Controls the AI response generation behavior
  llm:
    # Model identifier - must match deployed NIM
    # Currently deployed: meta/llama-3.1-8b-instruct (Llama 3.1 8B Instruct)
    model: "meta/llama-3.1-8b-instruct"
    
    temperature: "0.6"        # Controls randomness in responses
                              # 0.0 = Deterministic (same input → same output)
                              # 0.5-0.7 = Balanced (recommended for voice assistants)
                              # 1.0 = Creative (more varied, potentially inconsistent)
    
    maxTokens: "2048"         # Maximum length of generated response
                              # Increased to allow detailed explanations when asked
    
    # -------------------------------------------------------------------------
    # System Prompt for Voice Assistant
    # -------------------------------------------------------------------------
    # This prompt is critical for good voice interaction. Key considerations:
    # 1. NO MARKDOWN - TTS will read asterisks, hashes, etc. as literal text
    # 2. CONVERSATIONAL - Natural spoken language, not written format
    # 3. Q&A STYLE - Each interaction is a question-answer pair
    # 4. DETAIL ON REQUEST - Provide thorough explanations when asked
    # -------------------------------------------------------------------------
    systemPrompt: |
      You are an intelligent voice assistant powered by Llama 3.1. You are part of a real-time voice-to-voice conversation system where:
      
      1. The user speaks to you through a microphone (their speech is converted to text via ASR)
      2. You generate a text response
      3. Your response is converted to speech and played back to the user (via TTS)
      
      CRITICAL FORMATTING RULES (your response will be spoken aloud):
      - NEVER use markdown formatting (no asterisks, hashes, backticks, or special characters)
      - NEVER use bullet points or numbered lists with symbols
      - NEVER use code blocks or technical formatting
      - Instead of "**bold**" just say the word naturally
      - Instead of lists, use natural phrases like "First... Second... Third..." or "There are three things to consider..."
      - Write numbers as words when spoken naturally (say "three" not "3" for small numbers)
      - Avoid abbreviations that sound awkward when spoken
      
      CONVERSATION STYLE:
      - This is a voice conversation, so be natural and conversational
      - Each exchange is a question-answer pair - the user asks, you respond
      - Listen carefully to what the user is asking
      - Be warm, friendly, and helpful
      - Use natural speech patterns and transitions
      
      RESPONSE LENGTH:
      - For simple questions: Give concise, direct answers (1-3 sentences)
      - For complex questions or when user asks for detail: Provide thorough explanations
      - If the user says "explain more", "tell me more", or "go into detail", give comprehensive responses
      - Structure longer responses with clear verbal transitions ("First...", "Additionally...", "Finally...")
      
      HANDLING UNCERTAINTY:
      - If you don't understand the question, ask for clarification naturally
      - If you're not sure about something, say so honestly
      - Don't make up information
      
      Remember: Everything you write will be read aloud. Write as if you're speaking to a friend.

  # ---------------------------------------------------------------------------
  # TTS (Text-to-Speech) Configuration
  # ---------------------------------------------------------------------------
  # Controls how generated text is converted to audio
  tts:
    voice: ""                 # Voice name/ID for synthesis
                              # Empty string = use default voice for language
                              # Specific voices depend on deployed TTS model
    sampleRate: "16000"       # Output audio sample rate in Hz
                              # Should match client playback capabilities

# =============================================================================
# GATEWAY SERVICE CONFIGURATION
# =============================================================================
# Configuration for the Voice Gateway (our Python gRPC service)
# -----------------------------------------------------------------------------
gateway:
  # ---------------------------------------------------------------------------
  # Container Image
  # ---------------------------------------------------------------------------
  image: docker.io/sagdesai/voice-gateway:latest
  imagePullPolicy: Always     # Always = pull latest (dev), IfNotPresent (prod)
  
  # ---------------------------------------------------------------------------
  # Scaling
  # ---------------------------------------------------------------------------
  replicas: 1                 # Number of gateway pod replicas
                              # For HA: set to 2-3 and adjust PDB accordingly
  
  # ---------------------------------------------------------------------------
  # Service Configuration
  # ---------------------------------------------------------------------------
  service:
    type: ClusterIP           # ClusterIP = internal only (use with Ingress)
                              # LoadBalancer = external IP (cloud providers)
                              # NodePort = expose on node ports
    port: 50051               # gRPC service port
  
  # ---------------------------------------------------------------------------
  # Metrics Configuration (Prometheus)
  # ---------------------------------------------------------------------------
  # Prometheus metrics are exposed on a SEPARATE port from gRPC.
  # This ensures ZERO latency impact on the main voice flow.
  #
  # Available metrics:
  #   - voice_gateway_asr_latency_seconds (histogram)
  #   - voice_gateway_llm_ttft_seconds (histogram) - Time to First Token
  #   - voice_gateway_llm_total_seconds (histogram)
  #   - voice_gateway_tts_latency_seconds (histogram)
  #   - voice_gateway_e2e_latency_seconds (histogram) - End-to-End
  #   - voice_gateway_requests_total (counter)
  #   - voice_gateway_active_streams (gauge)
  #   - voice_gateway_*_errors_total (counters)
  #
  # Prometheus will auto-discover via pod/service annotations.
  # ---------------------------------------------------------------------------
  metrics:
    port: 8080                # Metrics HTTP server port (separate from gRPC)
    
    # ServiceMonitor for Prometheus Operator (kube-prometheus-stack)
    # Enable this if using Prometheus Operator for auto-discovery
    serviceMonitor:
      enabled: true           # Enable for Prometheus Operator discovery
      namespace: "prometheus" # Namespace where ServiceMonitor is created
      release: "kube-prometheus-stack"  # MUST match your Prometheus Operator release name
      interval: "30s"         # Scrape interval
      scrapeTimeout: "10s"    # Scrape timeout
      additionalLabels: {}    # Additional labels for ServiceMonitor

  # ---------------------------------------------------------------------------
  # Environment Variables (Service Discovery)
  # ---------------------------------------------------------------------------
  env:
    # Riva ASR/TTS service endpoint (Kubernetes service DNS)
    rivaUri: "riva-api:50051"
    
    # NIM LLM service endpoint (OpenAI-compatible API)
    nimUrl: "http://meta-llama3-8b-instruct:8000/v1"

  # ---------------------------------------------------------------------------
  # Resource Management
  # ---------------------------------------------------------------------------
  # Kubernetes resource requests and limits for the gateway container.
  #
  # How to determine values:
  #   1. Deploy without limits, run load test
  #   2. Monitor with: kubectl top pods -n voice-workflow
  #   3. Set requests = observed usage, limits = 2-4x requests
  #
  # Current values based on observed usage (2025-12-24):
  #   - Idle: ~1m CPU, ~56Mi memory
  #   - Active (streaming): ~10-50m CPU, ~100-200Mi memory
  # ---------------------------------------------------------------------------
  resources:
    requests:
      # Guaranteed resources - used for scheduling decisions
      memory: "128Mi"         # 2x observed idle usage (safety margin)
      cpu: "50m"              # 50 millicores = 0.05 CPU cores
    limits:
      # Maximum allowed - container killed if exceeded (OOMKilled)
      memory: "512Mi"         # Headroom for concurrent sessions + spikes
      cpu: "500m"             # 0.5 CPU cores max

  # ---------------------------------------------------------------------------
  # Pod Disruption Budget (PDB)
  # ---------------------------------------------------------------------------
  # Protects pods from VOLUNTARY disruptions:
  #   ✓ Node drains (kubectl drain)
  #   ✓ Cluster upgrades
  #   ✓ Voluntary scaling down
  #   ✗ Does NOT protect from: Node crashes, OOM kills, hardware failures
  #
  # Configuration options:
  #   minAvailable: N     - At least N pods must always be running
  #   maxUnavailable: N   - At most N pods can be down simultaneously
  #
  # Recommendations by replica count:
  #   replicas: 1 → minAvailable: 0 (allow disruption, or don't enable PDB)
  #   replicas: 2 → minAvailable: 1 (always keep 1 running)
  #   replicas: 3 → minAvailable: 2 OR maxUnavailable: 1
  #
  # Testing PDB:
  #   kubectl drain <node> --ignore-daemonsets --pod-selector=app=voice-gateway
  #   Expected: "Cannot evict pod as it would violate the pod's disruption budget"
  # ---------------------------------------------------------------------------
  pdb:
    enabled: true
    minAvailable: 1           # With 1 replica, this blocks all voluntary evictions
                              # Set to 0 if you want to allow node drains
    # maxUnavailable: 1       # Alternative: use this for HA setups

  # ---------------------------------------------------------------------------
  # Graceful Shutdown
  # ---------------------------------------------------------------------------
  # When Kubernetes sends SIGTERM (pod termination), the gateway:
  #   1. Stops accepting NEW connections
  #   2. Waits for EXISTING requests to complete (up to shutdownGracePeriod)
  #   3. Forcefully terminates remaining connections
  #   4. Exits cleanly
  #
  # terminationGracePeriodSeconds should be >= shutdownGracePeriod + buffer
  # ---------------------------------------------------------------------------
  shutdownGracePeriod: "10"           # Seconds to wait for active requests
  terminationGracePeriodSeconds: 30   # K8s termination grace period
  logLevel: "INFO"                    # Logging level: DEBUG, INFO, WARNING, ERROR

  # ---------------------------------------------------------------------------
  # Security Context
  # ---------------------------------------------------------------------------
  # The gateway runs as a non-root user (UID 1000) for security.
  # This is enforced at both pod and container level.
  #
  # Security features enabled:
  #   - runAsNonRoot: true         (prevents running as root)
  #   - readOnlyRootFilesystem     (prevents writing to container filesystem)
  #   - allowPrivilegeEscalation: false
  #   - capabilities: drop ALL     (removes all Linux capabilities)
  #
  # Writable Volumes (required for readOnlyRootFilesystem):
  #   The deployment mounts emptyDir volumes at:
  #   - /tmp                      (Python/gRPC temp files)
  #   - /home/appuser/.cache      (Python cache files)
  #   
  #   This allows the application to write temp files while keeping
  #   the root filesystem read-only for security.
  #
  # If you need to disable (not recommended):
  #   securityContext:
  #     enabled: false
  # ---------------------------------------------------------------------------
  securityContext:
    enabled: true
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000

  # ---------------------------------------------------------------------------
  # Secret Management
  # ---------------------------------------------------------------------------
  # Sensitive configuration (API keys, credentials) should be stored in Secrets,
  # NOT in ConfigMaps (which are visible to anyone with namespace access).
  #
  # Options:
  #   1. Let Helm create secrets (set create: true, provide values)
  #      ⚠️  Not recommended for production - values visible in values.yaml
  #
  #   2. Use existing secret (set create: false, existingSecret: "name")
  #      ✓  Recommended - create secret manually or via external tool
  #
  #   3. External Secrets Operator (enterprise)
  #      Create ExternalSecret CR that syncs from Vault/AWS/GCP
  #
  # Creating secrets manually:
  #   kubectl create secret generic gateway-secrets \
  #     --from-literal=LLM_API_KEY=your-key \
  #     -n voice-workflow
  # ---------------------------------------------------------------------------
  secrets:
    create: false                     # Set to true to create secret from values below
    existingSecret: ""                # Name of existing secret to use
    # Values below only used if create: true
    # llmApiKey: ""                   # API key for LLM (if required)
    # ngcApiKey: ""                   # NGC API key
    # extra:                          # Additional key-value pairs
    #   CUSTOM_KEY: "value"

# =============================================================================
# EXTERNAL DEPENDENCIES
# =============================================================================
# These services are deployed separately (via k8s/infra/ manifests)
# The flags below control whether this Helm chart should deploy them
# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
# NVIDIA Riva (ASR/TTS)
# -----------------------------------------------------------------------------
# Deployed manually via: ./scripts/deploy_riva.sh
# Using chart: helm/riva-api with k8s/infra/riva-values.yaml
riva:
  enabled: false              # false = already deployed separately
  ngcSecret: "ngc-secret"
  service:
    type: ClusterIP
  modelRepoGenerator:
    modelDeployKey: "tlt_encode"
    ngcSecret: "ngc-secret"

# -----------------------------------------------------------------------------
# NVIDIA NIM (LLM)
# -----------------------------------------------------------------------------
# Deployed manually via: ./scripts/deploy_infra.sh
# Using manifest: k8s/infra/nim-llm.yaml
nim:
  enabled: false              # false = already deployed separately
  image: nvcr.io/nim/meta/llama3-8b-instruct:latest
  replicas: 1
  resources:
    limits:
      nvidia.com/gpu: 1       # GPU allocation for LLM inference
