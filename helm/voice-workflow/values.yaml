# =============================================================================
# Voice Workflow Helm Chart - Configuration Values
# =============================================================================
#
# This file contains all configurable values for the Voice-to-Voice Workflow.
# Modify these values to customize your deployment without changing templates.
#
# Usage:
#   helm upgrade --install voice-gateway helm/voice-workflow -n voice-workflow
#
# Override specific values:
#   helm upgrade ... --set config.llm.temperature=0.7
#   helm upgrade ... --set gateway.replicas=3
#
# =============================================================================

# -----------------------------------------------------------------------------
# Global Configuration
# -----------------------------------------------------------------------------
global:
  ngcSecret: "ngc-secret"   # Kubernetes secret containing NGC_API_KEY

# =============================================================================
# RUNTIME CONFIGURATION (ConfigMap)
# =============================================================================
# These values are injected as environment variables via ConfigMap.
# Changes here do NOT require rebuilding the container image.
# After changing, redeploy with: ./scripts/deploy_gateway.sh
# -----------------------------------------------------------------------------
config:
  # ---------------------------------------------------------------------------
  # ASR (Automatic Speech Recognition) Configuration
  # ---------------------------------------------------------------------------
  # Controls how the Riva ASR service processes incoming audio
  asr:
    language: "en-US"         # Language code for speech recognition
                              # Supported: en-US, hi-IN (requires Hindi model)
                              # Must match the deployed Riva ASR model
    sampleRate: "16000"       # Audio sample rate in Hz
                              # Standard: 16000 (telephony/voice), 44100 (music)
                              # Must match client recording settings

  # ---------------------------------------------------------------------------
  # LLM (Large Language Model) Configuration
  # ---------------------------------------------------------------------------
  # Controls the AI response generation behavior
  llm:
    model: "meta/llama-3.1-8b-instruct"   # Model identifier for NIM
                                           # This must match the deployed NIM model
    
    temperature: "0.5"        # Controls randomness in responses
                              # 0.0 = Deterministic (same input → same output)
                              # 0.5 = Balanced (recommended for voice assistants)
                              # 1.0 = Creative (more varied, potentially inconsistent)
    
    maxTokens: "1024"         # Maximum length of generated response
                              # Voice responses should be concise: 256-512 typical
                              # Longer for complex explanations: 1024-2048
    
    systemPrompt: "You are a helpful voice assistant. Keep responses concise and conversational."
                              # Instructions that shape the AI's personality/behavior
                              # Examples:
                              # - "You are a Hindi-speaking assistant. Respond in Hindi."
                              # - "You are a customer service agent for ACME Corp."
                              # - "Keep responses under 2 sentences."

  # ---------------------------------------------------------------------------
  # TTS (Text-to-Speech) Configuration
  # ---------------------------------------------------------------------------
  # Controls how generated text is converted to audio
  tts:
    voice: ""                 # Voice name/ID for synthesis
                              # Empty string = use default voice for language
                              # Specific voices depend on deployed TTS model
    sampleRate: "16000"       # Output audio sample rate in Hz
                              # Should match client playback capabilities

# =============================================================================
# GATEWAY SERVICE CONFIGURATION
# =============================================================================
# Configuration for the Voice Gateway (our Python gRPC service)
# -----------------------------------------------------------------------------
gateway:
  # ---------------------------------------------------------------------------
  # Container Image
  # ---------------------------------------------------------------------------
  image: docker.io/sagdesai/voice-gateway:latest
  imagePullPolicy: Always     # Always = pull latest (dev), IfNotPresent (prod)
  
  # ---------------------------------------------------------------------------
  # Scaling
  # ---------------------------------------------------------------------------
  replicas: 1                 # Number of gateway pod replicas
                              # For HA: set to 2-3 and adjust PDB accordingly
  
  # ---------------------------------------------------------------------------
  # Service Configuration
  # ---------------------------------------------------------------------------
  service:
    type: ClusterIP           # ClusterIP = internal only (use with Ingress)
                              # LoadBalancer = external IP (cloud providers)
                              # NodePort = expose on node ports
    port: 50051               # gRPC service port

  # ---------------------------------------------------------------------------
  # Environment Variables (Service Discovery)
  # ---------------------------------------------------------------------------
  env:
    # Riva ASR/TTS service endpoint (Kubernetes service DNS)
    rivaUri: "riva-api:50051"
    
    # NIM LLM service endpoint (OpenAI-compatible API)
    nimUrl: "http://meta-llama3-8b-instruct:8000/v1"

  # ---------------------------------------------------------------------------
  # Resource Management
  # ---------------------------------------------------------------------------
  # Kubernetes resource requests and limits for the gateway container.
  #
  # How to determine values:
  #   1. Deploy without limits, run load test
  #   2. Monitor with: kubectl top pods -n voice-workflow
  #   3. Set requests = observed usage, limits = 2-4x requests
  #
  # Current values based on observed usage (2025-12-24):
  #   - Idle: ~1m CPU, ~56Mi memory
  #   - Active (streaming): ~10-50m CPU, ~100-200Mi memory
  # ---------------------------------------------------------------------------
  resources:
    requests:
      # Guaranteed resources - used for scheduling decisions
      memory: "128Mi"         # 2x observed idle usage (safety margin)
      cpu: "50m"              # 50 millicores = 0.05 CPU cores
    limits:
      # Maximum allowed - container killed if exceeded (OOMKilled)
      memory: "512Mi"         # Headroom for concurrent sessions + spikes
      cpu: "500m"             # 0.5 CPU cores max

  # ---------------------------------------------------------------------------
  # Pod Disruption Budget (PDB)
  # ---------------------------------------------------------------------------
  # Protects pods from VOLUNTARY disruptions:
  #   ✓ Node drains (kubectl drain)
  #   ✓ Cluster upgrades
  #   ✓ Voluntary scaling down
  #   ✗ Does NOT protect from: Node crashes, OOM kills, hardware failures
  #
  # Configuration options:
  #   minAvailable: N     - At least N pods must always be running
  #   maxUnavailable: N   - At most N pods can be down simultaneously
  #
  # Recommendations by replica count:
  #   replicas: 1 → minAvailable: 0 (allow disruption, or don't enable PDB)
  #   replicas: 2 → minAvailable: 1 (always keep 1 running)
  #   replicas: 3 → minAvailable: 2 OR maxUnavailable: 1
  #
  # Testing PDB:
  #   kubectl drain <node> --ignore-daemonsets --pod-selector=app=voice-gateway
  #   Expected: "Cannot evict pod as it would violate the pod's disruption budget"
  # ---------------------------------------------------------------------------
  pdb:
    enabled: true
    minAvailable: 1           # With 1 replica, this blocks all voluntary evictions
                              # Set to 0 if you want to allow node drains
    # maxUnavailable: 1       # Alternative: use this for HA setups

  # ---------------------------------------------------------------------------
  # Graceful Shutdown
  # ---------------------------------------------------------------------------
  # When Kubernetes sends SIGTERM (pod termination), the gateway:
  #   1. Stops accepting NEW connections
  #   2. Waits for EXISTING requests to complete (up to shutdownGracePeriod)
  #   3. Forcefully terminates remaining connections
  #   4. Exits cleanly
  #
  # terminationGracePeriodSeconds should be >= shutdownGracePeriod + buffer
  # ---------------------------------------------------------------------------
  shutdownGracePeriod: "10"           # Seconds to wait for active requests
  terminationGracePeriodSeconds: 30   # K8s termination grace period
  logLevel: "INFO"                    # Logging level: DEBUG, INFO, WARNING, ERROR

  # ---------------------------------------------------------------------------
  # Security Context
  # ---------------------------------------------------------------------------
  # The gateway runs as a non-root user (UID 1000) for security.
  # This is enforced at both pod and container level.
  #
  # Security features enabled:
  #   - runAsNonRoot: true         (prevents running as root)
  #   - readOnlyRootFilesystem     (prevents writing to container filesystem)
  #   - allowPrivilegeEscalation: false
  #   - capabilities: drop ALL     (removes all Linux capabilities)
  #
  # Writable Volumes (required for readOnlyRootFilesystem):
  #   The deployment mounts emptyDir volumes at:
  #   - /tmp                      (Python/gRPC temp files)
  #   - /home/appuser/.cache      (Python cache files)
  #   
  #   This allows the application to write temp files while keeping
  #   the root filesystem read-only for security.
  #
  # If you need to disable (not recommended):
  #   securityContext:
  #     enabled: false
  # ---------------------------------------------------------------------------
  securityContext:
    enabled: true
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000

  # ---------------------------------------------------------------------------
  # Secret Management
  # ---------------------------------------------------------------------------
  # Sensitive configuration (API keys, credentials) should be stored in Secrets,
  # NOT in ConfigMaps (which are visible to anyone with namespace access).
  #
  # Options:
  #   1. Let Helm create secrets (set create: true, provide values)
  #      ⚠️  Not recommended for production - values visible in values.yaml
  #
  #   2. Use existing secret (set create: false, existingSecret: "name")
  #      ✓  Recommended - create secret manually or via external tool
  #
  #   3. External Secrets Operator (enterprise)
  #      Create ExternalSecret CR that syncs from Vault/AWS/GCP
  #
  # Creating secrets manually:
  #   kubectl create secret generic gateway-secrets \
  #     --from-literal=LLM_API_KEY=your-key \
  #     -n voice-workflow
  # ---------------------------------------------------------------------------
  secrets:
    create: false                     # Set to true to create secret from values below
    existingSecret: ""                # Name of existing secret to use
    # Values below only used if create: true
    # llmApiKey: ""                   # API key for LLM (if required)
    # ngcApiKey: ""                   # NGC API key
    # extra:                          # Additional key-value pairs
    #   CUSTOM_KEY: "value"

# =============================================================================
# EXTERNAL DEPENDENCIES
# =============================================================================
# These services are deployed separately (via k8s/infra/ manifests)
# The flags below control whether this Helm chart should deploy them
# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
# NVIDIA Riva (ASR/TTS)
# -----------------------------------------------------------------------------
# Deployed manually via: ./scripts/deploy_riva.sh
# Using chart: helm/riva-api with k8s/infra/riva-values.yaml
riva:
  enabled: false              # false = already deployed separately
  ngcSecret: "ngc-secret"
  service:
    type: ClusterIP
  modelRepoGenerator:
    modelDeployKey: "tlt_encode"
    ngcSecret: "ngc-secret"

# -----------------------------------------------------------------------------
# NVIDIA NIM (LLM)
# -----------------------------------------------------------------------------
# Deployed manually via: ./scripts/deploy_infra.sh
# Using manifest: k8s/infra/nim-llm.yaml
nim:
  enabled: false              # false = already deployed separately
  image: nvcr.io/nim/meta/llama3-8b-instruct:latest
  replicas: 1
  resources:
    limits:
      nvidia.com/gpu: 1       # GPU allocation for LLM inference
