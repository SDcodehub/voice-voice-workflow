# Voice Workflow Helm Chart - Default Values
# This is a YAML-formatted file.

# Global settings
global:
  imageRegistry: ""
  imagePullSecrets: []
  namespace: voice-workflow

# Namespace configuration
namespace:
  create: true
  name: voice-workflow

# NGC credentials for Riva and NIM
ngc:
  apiKey: ""
  secretName: ngc-secret
  existingSecret: ""

# =============================================================================
# NVIDIA Riva Server (ASR + TTS)
# =============================================================================
rivaServer:
  enabled: true
  
  image:
    repository: nvcr.io/nvidia/riva/riva-speech
    tag: "2.14.0"
    pullPolicy: IfNotPresent
  
  replicas: 1
  
  # Hindi models configuration
  models:
    asr:
      - name: riva-asr-conformer-hi-IN
        languageCode: hi-IN
        enableAutomaticPunctuation: true
    tts:
      - name: riva-tts-fastpitch-hi-IN
        languageCode: hi-IN
  
  resources:
    limits:
      nvidia.com/gpu: 1
      memory: 32Gi
      cpu: "8"
    requests:
      nvidia.com/gpu: 1
      memory: 16Gi
      cpu: "4"
  
  persistence:
    enabled: true
    storageClass: ""
    size: 100Gi
  
  nodeSelector:
    nvidia.com/gpu.present: "true"
  
  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule
  
  service:
    type: ClusterIP
    grpcPort: 50051
    httpPort: 8000

# =============================================================================
# NVIDIA NIM LLM Server
# =============================================================================
nimLlm:
  enabled: true
  
  image:
    repository: nvcr.io/nim/meta/llama-3.1-8b-instruct
    tag: "1.2.2"
    pullPolicy: IfNotPresent
  
  replicas: 1
  
  config:
    model: "meta/llama-3.1-8b-instruct"
    maxModelLen: "4096"
    gpuMemoryUtilization: "0.9"
  
  resources:
    limits:
      nvidia.com/gpu: 1
      memory: 48Gi
      cpu: "16"
    requests:
      nvidia.com/gpu: 1
      memory: 32Gi
      cpu: "8"
  
  persistence:
    enabled: true
    storageClass: ""
    size: 100Gi
  
  nodeSelector:
    nvidia.com/gpu.present: "true"
  
  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule
  
  service:
    type: ClusterIP
    port: 8000

# =============================================================================
# Voice Gateway Service
# =============================================================================
voiceGateway:
  enabled: true
  
  image:
    repository: voice-gateway
    tag: latest
    pullPolicy: IfNotPresent
  
  replicas: 3
  
  config:
    defaultLanguage: "hi-IN"
    supportedLanguages:
      - "hi-IN"
      - "en-US"
    sessionTimeoutSeconds: 3600
    maxConcurrentSessions: 10000
  
  resources:
    limits:
      memory: 1Gi
      cpu: "1000m"
    requests:
      memory: 512Mi
      cpu: "500m"
  
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 20
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80
  
  service:
    type: ClusterIP
    port: 8000

# =============================================================================
# ASR Service
# =============================================================================
asrService:
  enabled: true
  
  image:
    repository: asr-service
    tag: latest
    pullPolicy: IfNotPresent
  
  replicas: 2
  
  config:
    languageCode: "hi-IN"
    sampleRate: 16000
    enableAutomaticPunctuation: true
    streamingInterimResults: true
    grpcPoolSize: 10
  
  resources:
    limits:
      memory: 1Gi
      cpu: "1000m"
    requests:
      memory: 512Mi
      cpu: "500m"
  
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
  
  service:
    type: ClusterIP
    httpPort: 8001
    grpcPort: 50051

# =============================================================================
# LLM Service
# =============================================================================
llmService:
  enabled: true
  
  image:
    repository: llm-service
    tag: latest
    pullPolicy: IfNotPresent
  
  replicas: 2
  
  config:
    backend: "nvidia_nim"
    model: "meta/llama-3.1-8b-instruct"
    maxTokens: 512
    temperature: 0.7
    topP: 0.9
    enableCache: true
    cacheTtlSeconds: 3600
    maxHistoryTurns: 10
  
  resources:
    limits:
      memory: 1Gi
      cpu: "1000m"
    requests:
      memory: 512Mi
      cpu: "500m"
  
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
  
  service:
    type: ClusterIP
    httpPort: 8002
    grpcPort: 50052

# =============================================================================
# TTS Service
# =============================================================================
ttsService:
  enabled: true
  
  image:
    repository: tts-service
    tag: latest
    pullPolicy: IfNotPresent
  
  replicas: 2
  
  config:
    languageCode: "hi-IN"
    sampleRate: 22050
    grpcPoolSize: 10
    streamingChunkSize: 4096
  
  resources:
    limits:
      memory: 1Gi
      cpu: "1000m"
    requests:
      memory: 512Mi
      cpu: "500m"
  
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
  
  service:
    type: ClusterIP
    httpPort: 8003
    grpcPort: 50053

# =============================================================================
# Redis
# =============================================================================
redis:
  enabled: true
  architecture: standalone
  auth:
    enabled: false
  master:
    resources:
      limits:
        memory: 512Mi
        cpu: "500m"
      requests:
        memory: 256Mi
        cpu: "250m"

# =============================================================================
# Ingress
# =============================================================================
ingress:
  enabled: true
  className: nginx
  annotations:
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
    nginx.ingress.kubernetes.io/websocket-services: "voice-gateway"
    nginx.ingress.kubernetes.io/proxy-buffering: "off"
  hosts:
    - host: voice.example.com
      paths:
        - path: /
          pathType: Prefix
  tls: []

# =============================================================================
# Observability
# =============================================================================
observability:
  enabled: true
  
  prometheus:
    enabled: true
    serviceMonitor:
      enabled: true
      interval: 30s
  
  tracing:
    enabled: true
    otlpEndpoint: "http://otel-collector:4317"
  
  grafana:
    dashboards:
      enabled: true

# =============================================================================
# Pod Disruption Budget
# =============================================================================
podDisruptionBudget:
  enabled: true
  minAvailable: 1

